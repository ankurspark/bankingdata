Big Bank, Big Problem

A leading banking and credit card services provider is trying to use Hadoop
technologies to handle and analyze large amounts of data.

Currently, the organization has data in the RDBMS but wants to use the
Hadoop ecosystem for storage, archival, and analysis of large amounts of
data.

Creating the Hadoop Cluster and Deploying Test Codes

For any Big Data project, a proper environmental setup is mandatory. In
real world scenarios, a Hadoop cluster can consist of 40K clusters/nodes.
As it is not possible for us to build such a huge setup in an academic lab,
we will be going with minimal 2 node cluster and simulate the ingestion,
cleaning, analysis and send the report to the downstream team.
Create a Pseudo-Distributed Node Cluster and deploy all the test codes
using Maven and SBT.
